\chapter{Assessing the quality of an automatically generated playlist} 

\label{Chapter3} 

\lhead{Chapter 3. \emph{Assessing the quality of an automatically generated playlist}} 

\section{Evaluation conferences in MIR}

Evaluation of a retrieval system is a fundamental task in order to achieve continuous improvements of it on the basis of the results obtained. Evaluation of MIR systems in generally based on test collections \cite{sanderson10}, following the Cranfield paradigm that is traditionally employed in Text IR \cite{harman11} \cite{gomez14}. On the other hand, the Text IR has a long tradition of conferences devoted to the evalutation of information retrieval systems, such as Text REtrieval Conference (TREC) \cite{trec05} and Conferenceand Labs of Evaluation Forum (CLEF) \cite{clef00}, where research teams interested in participating in a specific task can use data published by organizers and submit the output data of their own information retrieval system. Results of submitted data are then compared and evaluated by organizers, and during the actual conference results are discussed with participants, thus encouraging sharing of new promising techniques and main concernings. \\
No such evaluation conference exists in MIR \cite{gomez14}. In 2000, the International Conference of Music Information Retrieval (ISMIR) series of conferences started, as the premier forum for research on MIR. The first edition\footnote{\url{http://ismir2000.ismir.net/}} was held at Plymouth, Massachusetts (USA), covering the following topics: 
\begin{itemize}
\item Estimating similarity of melodies and polyphonic music
\item Music representation and indexing
\item Problems of recognizing music optically and/or via audio
\item Routing and filtering for music
\item Building up music databases
\item Evaluation of music-IR systems
\item Intellectual property rights issues
\item User interfaces for music IR
\item Issues related to musical styles and genres
\item Language modeling for music
\item User needs and expectations
\end{itemize}
However, until year 2004, MIR systems were evaluated with self-made test collections: each research group was using different documents, queries and measures \cite{orio06}. The first step toward a common evaluation framework has been carried out by Music Technology Group\footnote{\url{http://mtg.upf.edu/}} of Universitat Pompeu Fabra (Barcelona), which hosted ISMIR in that year. The evaluation framework was called \textit{Audio Description Contest} and divided into six indipendent tasks\footnote{\url{http://ismir2004.ismir.net/}}: 
\begin{itemize}
\item \textit{Genre Classification}: label an unknown song with one out of six possible music genres
\item \textit{Artist Identification}: identify one artist given three of his songs, after training the system with seven more songs 
\item \textit{Rhythm Classification}: label audio signals with one out of eight rhythm classes (Samba, Slow Waltz, Viennese Waltz, Tango, Cha Cha, Rumba, Jive, Quickstep)
\item \textit{Tempo Induction}: induce the basic tempo (i.e. a scalar, in BPM) from audio signals
\item \textit{Melody Extraction}: main melody detection from polyphonic audio signal
\end{itemize}
In the first edition, participants had to submit their own algorithms instead of the output data. These algorithms would have been compiled and run by organizers, who would have finally published the results. There was general agreement on the benefit of doing so, but it was also clear that data on which the systems were tested should have been published before, so that researchers could test their systems before submission and improve them between editions. 
This has instead been possible thanks to the many efforts of Dr. J.S. Downie, who organized several workshops on MIR evaluation (with the purpose of collecting ideas and needs of researchers in MIR) and finally started the International Music Information Retrieval System Evaluation Laboratory (IMIRSEL) project\footnote{\url{http://www.music-ir.org/evaluation/}}. The aim of the project is to create an provide secure and easily accessible music collections for MIR evaluation. Furthermore, the role of participants was made more important, as they could propose a particular task, defining the final goal, providing the datasets for training and testing the results, and defining the measures by which the results would have been ranked. The first evaluation campaign based on IMIRSEL, called \textit{Music Information Retrieval Evaluation eXchange} (MIREX). was organized in the year 2005 in London, and the results were presented and discussed at the ISMIR of the same year. MIREX is still based on an algorithm-to-data paradigm: participants submite the code or binaries for their systems and IMIRSEL runs them with pertinent datasets. The ISMIR-MIREX is being held every year, with an increasing amount of retrieval tasks performed: since 2005, over 1500 different runs have been evaluated for 22 different tasks, making it the premier evaluation forum in MIR research. The next conference is scheduled at Malaga, 26-30 October 2015\footnote{\url{http://ismir2015.uma.es/}}. 

\section{Difficulties in the evaluation of MIR systems}
Several hassles specific only to Music IR have arised since the birth of this research field. The most important difference with Text IR lies in the availability of data: while textual documents are readily available for example on Internet, music files are protected by copyrights, and the expenses related to the use of such files would make it pratically impossible to create publicly accessible collections \cite{gomez14}. The result has been that research teams acquired their private collections of audio files; they then test and evaluate their system on this specific private collection, hence reducing both the reproducibility and the validity of the research. \\
Moreover, music is inherently more difficult than text, as it is composed of several facets (for instance timbre, rhythm, lyrics, etc.); the result is that a music piece is still perceived as the same one even after alteration of some facets, such as pitch or lyrics. \\ Finally, the size of music files is several order of magnitude larger than the one of text files, with the result of collections requiring large storage space.  \\
For all these reasons, the option of providing an entire public collection of music files to be used during evaluation is generally disregarded. The only viable alternative in many cases is to just provide a set of features computed by third parties, such as in the Latin Music Database \cite{latin08} or the Million Song Dataset \cite{million11}. The problem of this approach is that, not being provided of a direct access to the multimedia files, research teams are constrained to the use of provided features, hence not giving any room for the exploration of new features extractable from signals. 

\section{Evaluation of automatically generated playlists}
The coherence of the tracks is a typical quality criterion for playlists \cite{logan04}. Therefore, maximizing similarities between consecutive elements is an obvious strategy to generate and to evaluate playlists. In general, \textit{user satisfaction} should be considered the main criterion to assess the quality of an automatically generated playlist. However, many factors come into play to determine the user's satisfaction: for instance we should consider the extent to which the list matches its intended purpose, fulfills the characteristic desired by the user, or is in line with user's expectactions \cite{bonnin14} \cite{fields11}. Hence it is clear that the evaluation of automatically generated playlists could concern many different aspects; for this reason, several categories of evaluation have been studied. In \cite{mcfee11}, Mcfee and Lanckriet propose to organize evaluation approaches in three different categories: human evaluation, semantic cohesion, and sequence prediction. In \cite{bonnin14}, Bonnin and Jannach propose to generalize this approach into the subdivision of evaluation approaches in four more general categories: user studies, log analysis, objective measures and comparison with handcrafted playlists.
\begin{itemize}
\item \textit{User studies}. Allow to determine the \textit{perceived} quality of playlists with good confidence. In such studies, participants to the experiment listen to automatically generated playlist and submit surveys in order to leave a feedback about the perceived quality of the playlist. The main drawback of this kind of studies is that they are time consuming and expensive. For some experimental designs, participants are required to listen to the entire catalogue of music. Studies of this kind are generally based only on 10 to 20 participants; only few studies have involved a considerable amount of participants (for instance, in \cite{barrington09} Barrington et al. present the results of a study conducted over 185 subjects). Moreover, another drawback of this category of evaluation approach is that the results are difficult to reproduce, for the experiment is based on specific software application.
\item \textit{Log analysis}. Users' logs about listening and interaction are analyzed. Such logs contain information about how often each user listened to an automatically generated playlist. Furthermore, if the playback system implements a \textit{``like/dislike''} feature, additional data regarding the enjoyability of the playlist for the users is provided. The main advantage of this technique is that it doesn't require direct participation of subjects to the evaluation. However, this approach requires a platform sharing information regarding listening behavior: such platforms are usually closed and thus they don't share data.  
\item \textit{Objective measures}. With the term objective measures, Bonnin and Jannach refer to \textit{measures that can be automatically computed for a given playlist and that try to approximate a user-perceived subjective quality} \cite{bonnin14} \cite{cremonesi11}. Typical examples of subjective qualities are diversity or homogeneity of a playlist. Many ways for assessing the homogeneity of a playlist have been presented, such as the number of different artists or genres appearing in the playlist. Anyways, a single quality criterion might not be sufficient to assess the quality of a playlist \cite{bonnin14}: determining, for example, the homogeneity alone might not lead to a good evaluation, especially since some researches have shown that determining diversity is at least as important as homogeneity \cite{slaney06} \cite{lee11}. Other measures that have been explored are freshness, novelty of track, their familiarity with respect to a certain user, the consistency of the mood, and the smoothness of transitions \cite{bonnin14}.
\item \textit{Comparison with handcrafted playlists}. Estimates the ability of the algorithm to generate playlists similar to the ones generated by music enthusiasts. The advantage of this technique is that for it, two well known protocols can be used for this kind of evaluation: hit rates and average log-likelihood. 
\begin{enumerate}
\item Hit rates are frequently used in Information retrieval, and give the ratio between the number of times the system retrieves a ``good'' result over the number of overall tries. For applying this measure, the idea is to take a handcrafted playlist and ``hide'' some of the tracks. The playlist generation system is left to guess the hidden items. A limitation of this strategy is that is based on the assumption that the hidden items are the most relevant ones of the collection, for they are the one on which the system is tested. Clearly, other elements are just as important. 
\item Average log-likelihood ($All$) can be used to assess how likely a system is to generate the tracks of a given set of deemed positve playlists. This measure is computed as:
\begin{equation}
All(Train, Test) = \frac{1}{\norm{Test}} \sum\limits_{(h,t) \in Test}^{} \log (P_{Train} (t | h))
\end{equation}
where $P_{Train}(t|h)$ is the probability of observing $t$ given $h$ according to a model learned on $Train$. The possible values range from $- \infty$ to 0, therefore it only allows to compare results between each others without knowing if the best one is actually good. It can be seen as a complementary measure to hit rate.
\end{enumerate}
\end{itemize}
